{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iclab/minjun/minjun_tuning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading base model in 4bit if QLoRA is True...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Attaching LoRA adapter from output dir...\n",
      "[INFO] Model and tokenizer loaded.\n",
      "Assistant:  Based on the provided text, several indicators suggest that the individual may be experiencing anxiety rather than depression:\n",
      "\n",
      "1. **Concern about Safety**: The mention of feeling \"a little uncomfortable\" when entering their personal financial details online indicates an awareness of potential risks or threats (in this case, identity theft). This concern suggests heightened vigilance, which is often associated with anxiety.\n",
      "\n",
      "2. **Doubtfulness**: The phrase \"Am I being too doubtful?\" reflects self-reflection regarding one’s own emotional state. While uncertainty can accompany both anxiety and depressive states, excessive worry about safety and judgmental thoughts are more characteristic of anxiety disorders.\n",
      "\n",
      "3.\n",
      "\n",
      "Assistant:  To determine whether the described scenario indicates depression, we need to consider the context and specific behaviors presented by the patient. However, without explicit statements regarding feelings such as sadness, loss of interest in activities, changes in appetite or sleep patterns, fatigue, difficulty concentrating, or feelings of worthlessness or guilt, it's challenging to make a definitive diagnosis based solely on the title of the post (\"Dini Ticaret Hali\"). \n",
      "\n",
      "The word \"dini,\" translated to English, refers to religious matters, suggesting the topic might relate to spiritual concerns rather than psychological distress typically associated with depression. Therefore, without further detail about the content of the post—\n",
      "\n",
      "Assistant:  In analyzing the concept of \"escape,\" we're dealing with various forms of escape mechanisms that individuals use to cope with stressors, trauma, or other negative emotions. These can manifest physically, emotionally, mentally, or behaviorally.\n",
      "\n",
      "In the context of mental health, common types of escapes include:\n",
      "\n",
      "1. **Substance Abuse**: Using drugs or alcohol to temporarily forget about problems or alleviate pain.\n",
      "2. **Avoidance Behaviors**: Engaging in activities that distract oneself from stressful situations or emotions.\n",
      "3. **Daydreams/Imagination**: Focusing on fantasy worlds or scenarios to avoid reality.\n",
      "4. **Social Withdrawal**:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Original model + LoRA directory\n",
    "base_model_dir = \"/home/iclab/minjun/Tiny-Mental-LLM/pretrained_models/meta-llama_Llama-3.2-3B-Instruct\"\n",
    "lora_dir = \"Output/meta-llama-Llama-3.2-3B-Instruct-QLoRA/checkpoint\"\n",
    "\n",
    "use_qlora = False\n",
    "\n",
    "if use_qlora:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    bnb_config = None\n",
    "    torch_dtype = torch.bfloat16\n",
    "\n",
    "print(\"[INFO] Loading base model in 4bit if QLoRA is True...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "\n",
    "print(\"[INFO] Attaching LoRA adapter from output dir...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_dir,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# If necessary, merge LoRA\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir, truncation_side='left')\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"[INFO] Model and tokenizer loaded.\")\n",
    "\n",
    "\n",
    "def run_multi_turn_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to conduct a multi-turn conversation with the user.\n",
    "    - max_new_tokens: Maximum number of tokens to generate in a single response\n",
    "    - temperature: Sampling temperature (0~1, higher means more randomness)\n",
    "    - top_p: Consider only candidates whose cumulative probability is top_p or less during sampling (0~1)\n",
    "    - num_beams: Number of beams for beam search (1 for sampling, 2 or more for beam search)\n",
    "    - repetition_penalty: Penalty for repetition (1 or more)\n",
    "    \"\"\"\n",
    "\n",
    "    conversation_history = \"\"\n",
    "    model.eval()\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"User: \")\n",
    "        if user_message.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Exiting chat...\")\n",
    "            break\n",
    "\n",
    "        # Update conversation history\n",
    "        conversation_history += f\"\\nUser: {user_message}\\nAssistant: \"\n",
    "\n",
    "        # Tokenize + move to GPU\n",
    "        inputs = tokenizer(\n",
    "            conversation_history,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                num_beams=num_beams,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                do_sample=(num_beams == 1),  # Distinguish beam search/sampling\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        # Extract only new tokens, excluding the input part from the entire generated tokens\n",
    "        new_tokens = generated_outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        assistant_reply = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Add to conversation history\n",
    "        conversation_history += assistant_reply\n",
    "        print(f\"Assistant: {assistant_reply}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "run_multi_turn_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.5,\n",
    "    top_p=0.8,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "```\n",
    "\n",
    "        Too many current requests. Your queue position is 1. Please wait for a while or switch to other models for a smoother experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minjun_tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
